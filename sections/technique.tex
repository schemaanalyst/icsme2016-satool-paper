\section{The \sa~Data Generation Tool}\label{sec:technique}

% Give the basic explanation of how a schema problem can negatively influence a system

An error in the specification of the database schema may result in the corruption of the data state and the disruption
of a supported service.  Even though verifying the accuracy of the database schema is a critical step towards protecting
data integrity, using manually created test data to do so is often expensive and time
consuming~\cite{kapfhammer2013search}.  \sa~uses a search-based approach to generate test data automatically.

% Walk through all of the details in the figure that gives the high-level architecture of the presented tool

Figure~\ref{fig:schemaanalyst} provides a high-level overview of the tool presented in this paper.  After being given a
database schema as input, \sa~uses a coverage criterion to first create a collection of test requirements, or the
rules that the test data must try to fulfill.  For instance, a test requirement for the \texttt{Inventory} schema in
Figure~\ref{fig:schema} might be ``the \texttt{PRIMARY KEY} constraint on line three must be violated''. A coverage
criterion supports systematically generating test requirements.  One example of a coverage criterion is Integrity
Constraint Coverage (ICC)~\cite{mcminn2015effectiveness}. It leads to two test requirements for every integrity
constraint in the schema: one requiring that the constraint is satisfied and another necessitating its violation.
\sa~supports nine coverage criteria, as further explained in~\cite{mcminn2015effectiveness}.

% Further explain the process of test data generation with the presented tool

The presented tool creates test data to satisfy the test requirements using a test data generator; the default test data
generator used by \sa~is based on Korel's Alternating Variable Method (AVM)~\cite{Korel:AVM}. This data generator uses a
fitness function to evaluate how well the test data satisfies the requirements, thereby aiding it in producing test data
that satisfies more of the requirements. For example, \sa~generated the following \texttt{INSERT} statement to violate
the \texttt{PRIMARY KEY} constraint on line three in Figure~\ref{fig:schema}.

% Give the example of the INSERT statement that the presented tool generated

\vspace*{.1in}
\texttt{INSERT INTO} Inventory \texttt{VALUES} (NULL, \textquotesingle\textquotesingle, 0, 0);
\vspace*{.1in}

% Explain the test case that was generated by SchemaAnalyst

Since this data instance would set the value for the \texttt{id} field to be \texttt{NULL}, it violates the
\texttt{PRIMARY KEY} constraint and thus should be rejected by the schema. \sa~encodes this test case in the
well-established JUnit format that is commonly used by developers, thereby providing a way to apply \sa~to industrial
systems.  The JUnit test first runs the \texttt{INSERT} statement on an installed DBMS (e.g., \sqlite) and then asserts
that the \texttt{INSERT} statement was rejected by the schema. If this is the case, then the test passes.  If the schema
allows the bad data, then the test fails.

% Explain the mutation analysis system that is a part of SchemaAnalyst

Although not included in Figure~\ref{fig:schemaanalyst} due to space constraints, \sa~also includes features to evaluate
the quality of the generated test data. First, it calculates coverage, or the percentage of the requirements satisfied
by the test data.  Test data quality can also be measured using the provided mutation testing tools. When executed in
mutation testing mode, \sa~will generate mutant database schemas and compare the behavior of the test suite on the
original and mutant schemas. \sa~includes 14 different mutation operators that can be used to assess test suite
quality~\cite{wright2015mutation}.

% GMK NOTE: The diagram does not talk about the mutation analysis framework! So, I have smoothed this over by
% specifically declaring that it is not in the diagram and adding a further disclaimer.

\input{figures/sa-new}
